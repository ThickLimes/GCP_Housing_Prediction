# GCP_Housing_Prediction

This application was developed to be able to perform housing price predictions based on a variety of factors. These factors include everything from the amount of rooms to the size of the lot to the type of roof. The model was developed by using a dataset of housing prices from Ames, Iowa. This data set contains around 80 variables that are used in the model. The application is hosted in the Google Cloud Platform (GCP) which is a cloud service provided by Google. The code for this application is written in Python.

The purpose of this application is to provide assistance to home buyers and sellers. Deciding on the price of a home can be complicated and murky, so this application was developed to provide fair and accurate price prediction. It will be most valuable to use this prediction in negotiations to serve as a point of reference for the sale price. 
The model was developed using Google's AutoML Tables which handled the model development, processing and feature engineering. AutoML Tables is a managed tool in the GCP that allows users to automatically generate, test, and deploy machine learning models. AutoML Tables focuses primarily on working with data that is structured and tabular. It can retrieve data from Cloud Storage or from a Big Query database. It also is able to handle the most common data issues such as missing data and outliers It is deployed to a GCP account and has an R^2 score of around .83. This indicates the vast majority of the variation in price is explained by the model that was generated which indicates a high quality model. It also has a MAPE score of about 12%. This means that the model is on average only off by around 12%. This also indicates the model is high quality.The training data is stored in Cloud Storage. It is recommended that the model be periodically evaluated for accuracy. This is because housing prices can change over time as properties can appreciate or depreciate depending on circumstances. I would recommend this model be retrained with new data from Ames once a year to account for the year to year price changes. It can be evaluated through the AutoML Tables API or with Python script.

The model is able to serve out predictions via a flask server that is hosted in the Google App Engine. Flask is a lightweight micro web framework. It allows for applications to be called with requests to a particular web address. The WSGI for this application is gunicorn which helps to handle web traffic and multiple requests. It accepts json formatted data as a payload. It will serve out a prediction for the house price based on the input variables (example of the variables in the tes_ex2.json file). The application can currently accept one data for one prediction at a time and will serve out a predicted price for those inputs. GCP also provides a monitoring service for this application. 

The monitoring for this project is handled through the Cloud Monitoring system. There is a dedicated dashboard for this project which will track how much use it is receiving as well as how many resources it is consuming. The deployed version of the app is actually fairly small in resource requirements. The Cloud Monitoring system performs regular readiness checks on the application to ensure that it is still running properly and it is not encountering issues. Response latency and response codes can be viewed here.
The logs for this application are handled by the Cloud Operations API. The logs can be viewed through the Logs Explorer within Cloud Operations. The logs report the results of the automated health and readiness checks that are performed at regular intervals. The logs also report the frequency and results of any requests that are sent to the application. These logs can be queried and searched through Logs Explorer GUI in the GCP console. The data within the logs can be easily viewed in the GUI and a histogram of the frequency of the logs can be viewed. This is most helpful when attempting to identify issues with the application or how much traffic is occurring at certain times. This is the first place to look should issues arise with the application.

The development environment was primarily on a local machine. This was a Windows OS with Pycharm. A local environment requires additional credentials added to the code which are currently commented out. These credentials can be supplied as a service account credential file which is generated from the GCP. This service account will need to have permissions to access the AutoML API and the Cloud Storage API in order to properly function. It is not recommended that development not be conducted on a local machine due to the complications of requiring a credential file. It is recommended that a virtual machine instance be set up in the GCP to be used a development environment. A virtual machine in the GCP will not require a credential file and should be able to run the code as is. This avoids needing to make edits when moving code from development to production.
The CI testing for the application is handled by GitHub actions. Currently only 1 test is activated, however there is code for several more tests. The reason these are tests are not activated is because they require credentials to access the model and test the deployment and predictions. The credential file  was removed from GitHub in order to make it public for viewing due to Google security measures that advise against storing the key in a publicly visible location. Under normal production circumstances, the repo would be made private and the credential would be stored here so the extra tests could be run via Github actions. The tests serve a few important functions. The test_etl_ex.py script retrieves the training data from the github repo and verifies that it meets a certain length to help to ensure property training data has been supplied. The connected_test.py script then checks to see if the model is currently deployed in the AutoML API to ensure that it is active and able to receive data for predictions. Finally, the prediction_test.py file takes a test json file and sends it to the deployed model to receive a prediction. It then checks that the prediction matches a certain value which has been previously output by the same data. This helps to ensure that the model is still functioning correctly and returning appropriate predictions.

Deployment is handled through CloudBuild.  Cloud Build automatically deploys the changes to the production environment when commits are made to the main github repository. It is recommended that development be done in separate branches and only merged into the main when thoroughly tested. App deployment does take a few minutes so it's recommended that changes only be pushed during times when the application is not actively being used.

